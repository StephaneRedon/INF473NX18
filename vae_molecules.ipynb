{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vae_molecules.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VntdkdMuaRVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejWiguEHPxHn",
        "colab_type": "code",
        "outputId": "24a99cd7-afee-40a5-dbd4-279055b395e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "pip install memory_profiler"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: memory_profiler in /usr/local/lib/python3.6/dist-packages (0.57.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from memory_profiler) (5.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs5vEMruc084",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE7nsz6LgTp7",
        "colab_type": "code",
        "outputId": "5ab881c7-7c1d-4282-e796-350f1a662fa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l4uUDkiLJey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "P = \"/content/gdrive/My Drive/Colab Notebooks/Modal_VAE/DataP.dat\"\n",
        "PL = \"/content/gdrive/My Drive/Colab Notebooks/Modal_VAE/DataPL.dat\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D778Y-anRgij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, channels, latent_size):\n",
        "        super(VAE, self).__init__()\n",
        "        \n",
        "\n",
        "        self.ls = latent_size\n",
        "        self.channels = channels\n",
        "\n",
        "        #\n",
        "        self.conv3d_1 = nn.Conv3d(channels, 24, (4,4,4), stride=3, padding=1)\n",
        "        self.conv3d_2 = nn.Conv3d(24, 26, (4,4,4), stride=3, padding=1)\n",
        "        self.conv3d_3 = nn.Conv3d(26, 28, (4,4,4), stride=2, padding=1)\n",
        "        self.conv3d_4 = nn.Conv3d(28, 32, (4,4,4), stride=2, padding=1)\n",
        "        #self.pool_1 = nn.MaxPool3d((4,4,4), stride = 3)\n",
        "\n",
        "        #\n",
        "        self.convTrans3d_1 = nn.ConvTranspose3d(32, 28, (4,4,4), stride=2, padding=1)\n",
        "        self.convTrans3d_2 = nn.ConvTranspose3d(28, 26, (4,4,4), stride=2, padding=1, output_padding=1)\n",
        "        self.convTrans3d_3 = nn.ConvTranspose3d(26, 24, (4,4,4), stride=2, padding=1)\n",
        "        self.convTrans3d_4 = nn.ConvTranspose3d(24, channels, (4,4,4), stride=2, padding=1)\n",
        "        \n",
        "        #\n",
        "        \n",
        "        self.bn_1 = nn.BatchNorm3d(32)\n",
        "        self.bn_2 = nn.BatchNorm3d(64)\n",
        "        self.bn_3 = nn.BatchNorm3d(1)\n",
        "        \n",
        "        #\n",
        "        self.dropOut = nn.Dropout3d(0.5)\n",
        "        self.mu = nn.Linear(256, latent_size)\n",
        "        self.logvar = nn.Linear(256, latent_size)\n",
        "        self.p = nn.Linear(latent_size, 32*6*6*6)\n",
        "\n",
        "        #\n",
        "        self.softmin = nn.Softmin(1)\n",
        "    \n",
        "    def encode(self, in_data):\n",
        "\n",
        "        h = self.conv3d_1(in_data.float())\n",
        "        h = F.leaky_relu(h)\n",
        "        h = self.dropOut(h)\n",
        "\n",
        "        h = self.conv3d_2(h)\n",
        "        #h = self.bn_1(h)\n",
        "        h = F.leaky_relu(h)\n",
        "        h = self.dropOut(h)\n",
        "\n",
        "        h = self.conv3d_3(h)\n",
        "        h = F.leaky_relu(h)\n",
        "        h = self.dropOut(h)\n",
        "\n",
        "        h = self.conv3d_4(h)\n",
        "\n",
        "\n",
        "        h = h.reshape(h.shape[0], -1)\n",
        "        mu = self.mu(h)\n",
        "        log_var = self.logvar(h)\n",
        "          \n",
        "        #sampling and reparameterization for backprop\n",
        "        z = torch.randn(mu.shape, device = dev)\n",
        "        z = mu + torch.exp(log_var) * z\n",
        "        return z, mu, log_var\n",
        "    \n",
        "    def decode(self, h_data):\n",
        "        z = self.p(h_data)\n",
        "        z = z.reshape(z.shape[0], 32, 6, 6, 6)\n",
        "\n",
        "        z = self.convTrans3d_1(z)\n",
        "        #z = self.bn_1(z)\n",
        "        z = F.leaky_relu(z)\n",
        "        z = self.dropOut(z)\n",
        "\n",
        "        z = self.convTrans3d_2(z)\n",
        "        #z = self.bn_1(z)\n",
        "        z = F.leaky_relu(z)\n",
        "        z = self.dropOut(z)\n",
        "\n",
        "        z = self.convTrans3d_3(z)\n",
        "        z = F.leaky_relu(z)\n",
        "        z = self.dropOut(z)  \n",
        "        z = self.convTrans3d_4(z)\n",
        "        z = self.softmin(z)\n",
        "        return z\n",
        "                \n",
        "    def forward(self, in_data):\n",
        "        z, mu, log_var = self.encode(in_data)\n",
        "        z = self.decode(z)\n",
        "        return z, mu, log_var"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmqFV2h1VuAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_vae = VAE(21, 64).to(dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8XID1rPVvnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Kullback–Leibler divergence for N(mu, var^2) and N(0, I)\n",
        "def kl_divergence(mu, log_var):\n",
        "    return 0.5*( torch.sum( log_var.exp(), 1) + torch.sum(mu*mu, 1) - mu.shape[1] - torch.sum(log_var, 1) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq4wFrImVxUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#(Additional loss + Kullback–Leibler divergence) / batch\n",
        "def vae_loss(mu, log_var, pred, target):\n",
        "    bs = pred.shape[0]\n",
        "    kl_loss = kl_divergence(mu, log_var).mean()\n",
        "    ad_loss = F.l1_loss(pred, target, reduction = 'sum') / bs\n",
        "    return (kl_loss + ad_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwEuTy-BV1sN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.001\n",
        "epochs = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlps-i-SV42v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_func = vae_loss\n",
        "opt = torch.optim.Adam(model_vae.parameters(), lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWGQ8YD2XRHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit1(epochs, model, loss_func, opt, paths):\n",
        "    part_size = 5\n",
        "    total_size = 25\n",
        "    valid_size = 5\n",
        "    bs = 5\n",
        "    i = 0 #for offset\n",
        "    for epoch in range(epochs):\n",
        "      while (i <= (total_size - valid_size) * 100**3):\n",
        "        table_data = np.fromfile(paths[0],  dtype=np.int8, count=part_size * 100**3, offset = i)\n",
        "        table_data = table_data.reshape(part_size,100,100,100)\n",
        "        table_target = np.fromfile(paths[1],  dtype=np.int8, count=part_size * 100**3, offset = i)\n",
        "        table_target = table_target.reshape(part_size,100,100,100)\n",
        "\n",
        "        i += part_size * 100**3\n",
        "\n",
        "        train_data = torch.tensor(table_data, device=dev)\n",
        "        train_data = F.one_hot(train_data.long(), model.channels).to(dev)\n",
        "        train_data = torch.transpose(train_data, 1,4)\n",
        "\n",
        "        train_target = torch.tensor(table_target, dtype=torch.long, device=dev)\n",
        "        train_target = F.one_hot(train_target.long(), model.channels).to(dev)\n",
        "        train_target = torch.transpose(train_target, 1,4)\n",
        "\n",
        "        train_ds = TensorDataset(train_data, train_target)\n",
        "        del train_data, train_target\n",
        "        train_dl = DataLoader(train_ds, bs)\n",
        "\n",
        "  \n",
        "        model = model.train()\n",
        "        for data, target in train_dl:\n",
        "            pred, mu, log_var = model(data)\n",
        "            loss = loss_func(mu, log_var, pred, target)\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "      i = (total_size - valid_size) * 100**3\n",
        "      while (i < total_size * 100**3):\n",
        "        table_data = np.fromfile(paths[0],  dtype=np.int8, count=part_size * 100**3, offset = i)\n",
        "        table_data = table_data.reshape(part_size,100,100,100)\n",
        "        table_target = np.fromfile(paths[1],  dtype=np.int8, count=part_size * 100**3, offset = i)\n",
        "        table_target = table_target.reshape(part_size,100,100,100)\n",
        "\n",
        "        i += part_size * 100**3\n",
        "\n",
        "        valid_data = torch.tensor(table_data, device=dev)\n",
        "        valid_data = F.one_hot(valid_data.long(), model.channels).to(dev)\n",
        "        valid_data = torch.transpose(valid_data, 1,4)\n",
        "\n",
        "        valid_target = torch.tensor(table_target, dtype=torch.long, device=dev)\n",
        "        valid_target = F.one_hot(valid_target.long(), model.channels).to(dev)\n",
        "        valid_target = torch.transpose(valid_target, 1,4)\n",
        "\n",
        "        valid_ds = TensorDataset(valid_data, valid_target)\n",
        "        del valid_data, valid_target      \n",
        "        valid_dl = DataLoader(valid_ds, bs)\n",
        "      \n",
        "        model = model.eval()\n",
        "        with torch.no_grad():\n",
        "            average_valid_loss = torch.zeros(1)\n",
        "            #ad_loss = torch.zeros(1)\n",
        "            num_batch = 0\n",
        "            for data, target in valid_dl:\n",
        "                pred, mu, log_var = model(data)\n",
        "                average_valid_loss += loss_func(mu, log_var, pred, target)\n",
        "                num_batch += 1\n",
        "                #ad_loss += F.cross_entropy(pred, target, reduction='sum')\n",
        "            average_valid_loss /= num_batch\n",
        "            #ad_loss /= (bs * num_batch)\n",
        "            #if (epoch % 100 == 1):\n",
        "            print(\"Epoch: \" + str(epoch) + \"  Cross entropy + KL-divergence: \" + str(average_valid_loss)) #+ \"  Cross entropy: \" + str(ad_loss))\n",
        "\n",
        "      #del train_dl, valid_dl, train_ds, valid_ds, train_data, valid_data, train_target, valid_target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TzlcxaZytM8",
        "colab_type": "code",
        "outputId": "dc0e1e87-7f20-4a68-9b93-85d8f749b290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "fit1(epochs, model_vae, loss_func, opt, (P, PL))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0  Cross entropy + KL-divergence: tensor([1899151.5000])\n",
            "Epoch: 1  Cross entropy + KL-divergence: tensor([1899306.8750])\n",
            "Epoch: 2  Cross entropy + KL-divergence: tensor([1899281.5000])\n",
            "Epoch: 3  Cross entropy + KL-divergence: tensor([1899398.])\n",
            "Epoch: 4  Cross entropy + KL-divergence: tensor([1899378.8750])\n",
            "Epoch: 5  Cross entropy + KL-divergence: tensor([1899061.8750])\n",
            "Epoch: 6  Cross entropy + KL-divergence: tensor([1899258.6250])\n",
            "Epoch: 7  Cross entropy + KL-divergence: tensor([1899199.5000])\n",
            "Epoch: 8  Cross entropy + KL-divergence: tensor([1899036.])\n",
            "Epoch: 9  Cross entropy + KL-divergence: tensor([1898974.6250])\n",
            "Epoch: 10  Cross entropy + KL-divergence: tensor([1899261.])\n",
            "Epoch: 11  Cross entropy + KL-divergence: tensor([1899375.])\n",
            "Epoch: 12  Cross entropy + KL-divergence: tensor([1899481.6250])\n",
            "Epoch: 13  Cross entropy + KL-divergence: tensor([1899436.5000])\n",
            "Epoch: 14  Cross entropy + KL-divergence: tensor([1899484.6250])\n",
            "Epoch: 15  Cross entropy + KL-divergence: tensor([1899308.6250])\n",
            "Epoch: 16  Cross entropy + KL-divergence: tensor([1899205.2500])\n",
            "Epoch: 17  Cross entropy + KL-divergence: tensor([1899312.2500])\n",
            "Epoch: 18  Cross entropy + KL-divergence: tensor([1899318.])\n",
            "Epoch: 19  Cross entropy + KL-divergence: tensor([1898920.8750])\n",
            "Epoch: 20  Cross entropy + KL-divergence: tensor([1899437.8750])\n",
            "Epoch: 21  Cross entropy + KL-divergence: tensor([1899107.])\n",
            "Epoch: 22  Cross entropy + KL-divergence: tensor([1899247.5000])\n",
            "Epoch: 23  Cross entropy + KL-divergence: tensor([1899065.2500])\n",
            "Epoch: 24  Cross entropy + KL-divergence: tensor([1899522.5000])\n",
            "Epoch: 25  Cross entropy + KL-divergence: tensor([1899052.5000])\n",
            "Epoch: 26  Cross entropy + KL-divergence: tensor([1899303.5000])\n",
            "Epoch: 27  Cross entropy + KL-divergence: tensor([1899348.6250])\n",
            "Epoch: 28  Cross entropy + KL-divergence: tensor([1899365.8750])\n",
            "Epoch: 29  Cross entropy + KL-divergence: tensor([1899104.6250])\n",
            "Epoch: 30  Cross entropy + KL-divergence: tensor([1899303.5000])\n",
            "Epoch: 31  Cross entropy + KL-divergence: tensor([1899377.5000])\n",
            "Epoch: 32  Cross entropy + KL-divergence: tensor([1899294.6250])\n",
            "Epoch: 33  Cross entropy + KL-divergence: tensor([1899251.5000])\n",
            "Epoch: 34  Cross entropy + KL-divergence: tensor([1899037.])\n",
            "Epoch: 35  Cross entropy + KL-divergence: tensor([1899541.2500])\n",
            "Epoch: 36  Cross entropy + KL-divergence: tensor([1899502.5000])\n",
            "Epoch: 37  Cross entropy + KL-divergence: tensor([1899473.8750])\n",
            "Epoch: 38  Cross entropy + KL-divergence: tensor([1899360.8750])\n",
            "Epoch: 39  Cross entropy + KL-divergence: tensor([1899362.])\n",
            "Epoch: 40  Cross entropy + KL-divergence: tensor([1898891.])\n",
            "Epoch: 41  Cross entropy + KL-divergence: tensor([1899208.6250])\n",
            "Epoch: 42  Cross entropy + KL-divergence: tensor([1899119.])\n",
            "Epoch: 43  Cross entropy + KL-divergence: tensor([1899505.2500])\n",
            "Epoch: 44  Cross entropy + KL-divergence: tensor([1899409.5000])\n",
            "Epoch: 45  Cross entropy + KL-divergence: tensor([1899118.2500])\n",
            "Epoch: 46  Cross entropy + KL-divergence: tensor([1899376.2500])\n",
            "Epoch: 47  Cross entropy + KL-divergence: tensor([1899327.8750])\n",
            "Epoch: 48  Cross entropy + KL-divergence: tensor([1899233.])\n",
            "Epoch: 49  Cross entropy + KL-divergence: tensor([1899169.])\n",
            "Epoch: 50  Cross entropy + KL-divergence: tensor([1899388.5000])\n",
            "Epoch: 51  Cross entropy + KL-divergence: tensor([1899273.8750])\n",
            "Epoch: 52  Cross entropy + KL-divergence: tensor([1899295.5000])\n",
            "Epoch: 53  Cross entropy + KL-divergence: tensor([1899461.5000])\n",
            "Epoch: 54  Cross entropy + KL-divergence: tensor([1899099.6250])\n",
            "Epoch: 55  Cross entropy + KL-divergence: tensor([1899458.])\n",
            "Epoch: 56  Cross entropy + KL-divergence: tensor([1899244.8750])\n",
            "Epoch: 57  Cross entropy + KL-divergence: tensor([1899118.2500])\n",
            "Epoch: 58  Cross entropy + KL-divergence: tensor([1899149.2500])\n",
            "Epoch: 59  Cross entropy + KL-divergence: tensor([1898793.5000])\n",
            "Epoch: 60  Cross entropy + KL-divergence: tensor([1899444.2500])\n",
            "Epoch: 61  Cross entropy + KL-divergence: tensor([1899378.])\n",
            "Epoch: 62  Cross entropy + KL-divergence: tensor([1899519.])\n",
            "Epoch: 63  Cross entropy + KL-divergence: tensor([1899327.])\n",
            "Epoch: 64  Cross entropy + KL-divergence: tensor([1899276.8750])\n",
            "Epoch: 65  Cross entropy + KL-divergence: tensor([1899471.5000])\n",
            "Epoch: 66  Cross entropy + KL-divergence: tensor([1899029.])\n",
            "Epoch: 67  Cross entropy + KL-divergence: tensor([1899528.8750])\n",
            "Epoch: 68  Cross entropy + KL-divergence: tensor([1899552.8750])\n",
            "Epoch: 69  Cross entropy + KL-divergence: tensor([1899282.])\n",
            "Epoch: 70  Cross entropy + KL-divergence: tensor([1899324.6250])\n",
            "Epoch: 71  Cross entropy + KL-divergence: tensor([1899397.8750])\n",
            "Epoch: 72  Cross entropy + KL-divergence: tensor([1899515.6250])\n",
            "Epoch: 73  Cross entropy + KL-divergence: tensor([1899236.8750])\n",
            "Epoch: 74  Cross entropy + KL-divergence: tensor([1899171.5000])\n",
            "Epoch: 75  Cross entropy + KL-divergence: tensor([1899401.])\n",
            "Epoch: 76  Cross entropy + KL-divergence: tensor([1899356.8750])\n",
            "Epoch: 77  Cross entropy + KL-divergence: tensor([1899273.5000])\n",
            "Epoch: 78  Cross entropy + KL-divergence: tensor([1899320.5000])\n",
            "Epoch: 79  Cross entropy + KL-divergence: tensor([1899160.2500])\n",
            "Epoch: 80  Cross entropy + KL-divergence: tensor([1899169.8750])\n",
            "Epoch: 81  Cross entropy + KL-divergence: tensor([1899334.])\n",
            "Epoch: 82  Cross entropy + KL-divergence: tensor([1899051.5000])\n",
            "Epoch: 83  Cross entropy + KL-divergence: tensor([1899183.5000])\n",
            "Epoch: 84  Cross entropy + KL-divergence: tensor([1899175.8750])\n",
            "Epoch: 85  Cross entropy + KL-divergence: tensor([1899581.])\n",
            "Epoch: 86  Cross entropy + KL-divergence: tensor([1899302.6250])\n",
            "Epoch: 87  Cross entropy + KL-divergence: tensor([1899476.2500])\n",
            "Epoch: 88  Cross entropy + KL-divergence: tensor([1899473.])\n",
            "Epoch: 89  Cross entropy + KL-divergence: tensor([1899137.2500])\n",
            "Epoch: 90  Cross entropy + KL-divergence: tensor([1899209.2500])\n",
            "Epoch: 91  Cross entropy + KL-divergence: tensor([1899179.])\n",
            "Epoch: 92  Cross entropy + KL-divergence: tensor([1899515.6250])\n",
            "Epoch: 93  Cross entropy + KL-divergence: tensor([1899413.8750])\n",
            "Epoch: 94  Cross entropy + KL-divergence: tensor([1899513.5000])\n",
            "Epoch: 95  Cross entropy + KL-divergence: tensor([1899630.])\n",
            "Epoch: 96  Cross entropy + KL-divergence: tensor([1899457.])\n",
            "Epoch: 97  Cross entropy + KL-divergence: tensor([1899663.2500])\n",
            "Epoch: 98  Cross entropy + KL-divergence: tensor([1899206.])\n",
            "Epoch: 99  Cross entropy + KL-divergence: tensor([1899145.8750])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI7_PVNnwjTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}